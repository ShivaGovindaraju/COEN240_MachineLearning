{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Classification\n",
    "\n",
    "The activity shows an example of designing and training a neural network model using TensorFlow.\n",
    "\n",
    "Reference: https://adventuresinmachinelearning.com/neural-networks-tutorial/, https://www.tensorflow.org/tutorials/quickstart/beginner, https://www.tensorflow.org/tutorials/quickstart/advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as r\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset, show data\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[0]) \n",
    "plt.show()\n",
    "digits.data[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , -0.33501649, -0.04308102,  0.27407152, -0.66447751,\n",
       "       -0.84412939, -0.40972392, -0.12502292, -0.05907756, -0.62400926,\n",
       "        0.4829745 ,  0.75962245, -0.05842586,  1.12772113,  0.87958306,\n",
       "       -0.13043338, -0.04462507,  0.11144272,  0.89588044, -0.86066632,\n",
       "       -1.14964846,  0.51547187,  1.90596347, -0.11422184, -0.03337973,\n",
       "        0.48648928,  0.46988512, -1.49990136, -1.61406277,  0.07639777,\n",
       "        1.54181413, -0.04723238,  0.        ,  0.76465553,  0.05263019,\n",
       "       -1.44763006, -1.73666443,  0.04361588,  1.43955804,  0.        ,\n",
       "       -0.06134367,  0.8105536 ,  0.63011714, -1.12245711, -1.06623158,\n",
       "        0.66096475,  0.81845076, -0.08874162, -0.03543326,  0.74211893,\n",
       "        1.15065212, -0.86867056,  0.11012973,  0.53761116, -0.75743581,\n",
       "       -0.20978513, -0.02359646, -0.29908135,  0.08671869,  0.20829258,\n",
       "       -0.36677122, -1.14664746, -0.5056698 , -0.19600752])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale data\n",
    "X_scale = StandardScaler()\n",
    "X = X_scale.fit_transform(digits.data)\n",
    "X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train/test\n",
    "y = digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert output to binary (1-hot encoding) vector\n",
    "def convert_y_to_vect(y):\n",
    "    y_vect = np.zeros((len(y), 10))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, y[i]] = 1\n",
    "    return y_vect\n",
    "y_v_train = convert_y_to_vect(y_train)\n",
    "y_v_test = convert_y_to_vect(y_test)\n",
    "y_train[0], y_v_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the neural network\n",
    "nn_structure = [64, 30, 10]\n",
    "def f(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def f_deriv(x):\n",
    "    return f(x) * (1 - f(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent algorithm works as follows:\n",
    "\n",
    "Randomly initialise the weights for each layer $W^{(l)}$.  \n",
    "While iterations < iteration limit:  \n",
    "1. Set $\\Delta W$ and $\\Delta b$ to zero.  \n",
    "2. For samples 1 to m:  \n",
    "   a. Perform a feed foward pass through all the $n_l$ layers. Store the activation function outputs $h^{(l)}$.  \n",
    "   b. Calculate the $\\delta^{(n_l)}$ value for the output layer.  \n",
    "   c. Use backpropagation to calculate the $\\delta^{(l)}$ values for layers 2 to $n_l-1$.  \n",
    "   d. Update the $\\Delta W^{(l)}$ and $\\Delta b^{(l)}$  for each layer.  \n",
    "3. Perform a gradient descent step using:  \n",
    "$W^{(l)} = W^{(l)}-\\alpha[\\frac{1}{m}\\Delta W^{(l)}]$,  \n",
    "$b^{(l)} = b^{(l)}-\\alpha[\\frac{1}{m}\\Delta b^{(l)}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters to random values\n",
    "def setup_and_init_weights(nn_structure):\n",
    "    W = {}\n",
    "    b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1]))\n",
    "        b[l] = r.random_sample((nn_structure[l],))\n",
    "    return W, b\n",
    "\n",
    "def init_tri_values(nn_structure):\n",
    "    tri_W = {}\n",
    "    tri_b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        tri_W[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "        tri_b[l] = np.zeros((nn_structure[l],))\n",
    "    return tri_W, tri_b\n",
    "\n",
    "def feed_forward(x, W, b):\n",
    "    h = {1: x}\n",
    "    z = {}\n",
    "    for l in range(1, len(W) + 1):\n",
    "        # if it is the first layer, then the input into the weights is x, otherwise, \n",
    "        # it is the output from the last layer\n",
    "        if l == 1:\n",
    "            node_in = x\n",
    "        else:\n",
    "            node_in = h[l]\n",
    "        z[l+1] = W[l].dot(node_in) + b[l] # z^(l+1) = W^(l)*h^(l) + b^(l)  \n",
    "        h[l+1] = f(z[l+1]) # h^(l) = f(z^(l)) \n",
    "    return h, z\n",
    "\n",
    "def calculate_out_layer_delta(y, h_out, z_out):\n",
    "    # delta^(nl) = -(y_i - h_i^(nl)) * f'(z_i^(nl))\n",
    "    return -(y-h_out) * f_deriv(z_out)\n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "    return np.dot(np.transpose(w_l), delta_plus_1) * f_deriv(z_l)\n",
    "\n",
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    m = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%1000 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        for i in range(len(y)):\n",
    "            delta = {}\n",
    "            # perform the feed forward pass and return the stored h and z values, to be used in the\n",
    "            # gradient descent step\n",
    "            h, z = feed_forward(X[i, :], W, b)\n",
    "            # loop from nl-1 to 1 backpropagating the errors\n",
    "            for l in range(len(nn_structure), 0, -1):\n",
    "                if l == len(nn_structure):\n",
    "                    delta[l] = calculate_out_layer_delta(y[i,:], h[l], z[l])\n",
    "                    avg_cost += np.linalg.norm((y[i,:]-h[l]))\n",
    "                else:\n",
    "                    if l > 1:\n",
    "                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(h^(l))\n",
    "                    tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(h[l][:,np.newaxis])) \n",
    "                    # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                    tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            W[l] += -alpha * (1.0/m * tri_W[l])\n",
    "            b[l] += -alpha * (1.0/m * tri_b[l])\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0/m * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 3000 iterations\n",
      "Iteration 0 of 3000\n",
      "Iteration 1000 of 3000\n",
      "Iteration 2000 of 3000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 44s, sys: 383 ms, total: 2min 44s\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train model and plot cost function\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train, y_v_train)\n",
    "plt.plot(avg_cost_func)\n",
    "plt.ylabel('Average J')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.17802503477051"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assess model performance\n",
    "def predict_y(W, b, X, n_layers):\n",
    "    m = X.shape[0]\n",
    "    y = np.zeros((m,))\n",
    "    for i in range(m):\n",
    "        h, z = feed_forward(X[i, :], W, b)\n",
    "        y[i] = np.argmax(h[n_layers])\n",
    "    return y\n",
    "\n",
    "y_pred = predict_y(W, b, X_test, 3)\n",
    "accuracy_score(y_test, y_pred)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow beginner version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.72303219e-01,  5.04035710e-01, -2.36671756e-01,\n",
       "        -1.63559060e-01, -2.40954505e-01, -4.37905302e-01,\n",
       "        -2.98322534e-04,  9.52206843e-02,  6.72229779e-02,\n",
       "        -4.81110492e-01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# load and split dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Stack the layers\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# For each example the model returns a vector of \"logits\" or \"log-odds\" scores, one for each class.\n",
    "predictions = model(x_train[:1]).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12247447, 0.17065348, 0.08136357, 0.08753514, 0.08101585,\n",
       "        0.06653273, 0.10305894, 0.11338851, 0.11025792, 0.06371939]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tf.nn.softmax function converts these logits to \"probabilities\" for each class\n",
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7100613117218018"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The losses.SparseCategoricalCrossentropy loss takes a vector of logits and a True index and returns a scalar loss for each example.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_fn(y_train[:1], predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.2997 - accuracy: 0.9120\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.1450 - accuracy: 0.9570\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1086 - accuracy: 0.9675\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0876 - accuracy: 0.9728\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0751 - accuracy: 0.9767\n",
      "CPU times: user 27 s, sys: 3.61 s, total: 30.7 s\n",
      "Wall time: 8.81 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc930260cd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# compile and fit/learn the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 0s - loss: 0.0694 - accuracy: 0.9788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06937223155732469, 0.9788]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Load and prepare the MNIST dataset.\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]\n",
    "\n",
    "# Use tf.data to batch and shuffle the dataset:\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "# Build the tf.keras model using the Keras model subclassing API\n",
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(128, activation='relu')\n",
    "        self.d2 = Dense(10)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()\n",
    "\n",
    "# Choose an optimizer and loss function for training\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Select metrics to measure the loss and the accuracy of the model.\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "# Use tf.GradientTape to train the model\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "    \n",
    "# test the model\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1345686917601774, Accuracy: 95.97166666666666, Test Loss: 0.0620757358452936, Test Accuracy: 97.91\n",
      "Epoch 2, Loss: 0.045320465070350716, Accuracy: 98.61333333333333, Test Loss: 0.05382331483114061, Test Accuracy: 98.24000000000001\n",
      "Epoch 3, Loss: 0.024378025516072134, Accuracy: 99.20833333333333, Test Loss: 0.05122035293223484, Test Accuracy: 98.45\n",
      "Epoch 4, Loss: 0.01498234855302629, Accuracy: 99.5, Test Loss: 0.0567034374064936, Test Accuracy: 98.4\n",
      "Epoch 5, Loss: 0.010724055497633648, Accuracy: 99.63, Test Loss: 0.06274510387954933, Test Accuracy: 98.39\n",
      "CPU times: user 26min 3s, sys: 1min 14s, total: 27min 17s\n",
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# do the actual training\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch + 1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result() * 100,\n",
    "                          test_loss.result(),\n",
    "                          test_accuracy.result() * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
